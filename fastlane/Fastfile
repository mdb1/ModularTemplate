default_platform(:ios)

# Custom method to print minimal output
def minimal_output(message)
  puts message
end

# Custom method to suppress fastlane output for minimal verbosity
def suppress_output
  original_stdout = $stdout.clone
  original_stderr = $stderr.clone
  
  $stdout.reopen(File.new('/dev/null', 'w'))
  $stderr.reopen(File.new('/dev/null', 'w'))
  
  yield
  
  $stdout.reopen(original_stdout)
  $stderr.reopen(original_stderr)
end

# Custom method to extract code coverage from xcresult bundle
def extract_coverage_from_xcresult(result_bundle_path)
  return nil unless File.exist?(result_bundle_path)
  
  begin
    # Get the coverage report
    cmd = "xcrun xccov view --report --json #{result_bundle_path}"
    json_output = `#{cmd}`
    
    # Check if we have output
    return nil if json_output.empty?
    
    # Extract just the JSON part (sometimes there might be other output before/after)
    json_match = json_output.match(/\{.*\}/m)
    return nil unless json_match
    
    json_data = json_match[0]
    report_json = JSON.parse(json_data)
    
    # Initialize counters
    total_source_coverable_lines = 0
    total_source_covered_lines = 0
    
    # Process all targets in the report
    targets = report_json["targets"] || []
    targets.each do |target|
      # Skip test targets or targets with no executable lines
      next if target["name"].include?("Tests") || target["executableLines"].to_i == 0
      
      # Process files
      files = target["files"] || []
      files.each do |file|
        # Skip test files and mocks
        file_path = file["path"] || ""
        next if file_path.include?("Tests/") || file_path.include?("XCTest") || 
                file_path.include?("Mock") || file_path.include?("Stub")
        
        # Count lines for source files
        coverable_lines = file["executableLines"] || 0
        covered_lines = file["coveredLines"] || 0
        
        total_source_coverable_lines += coverable_lines
        total_source_covered_lines += covered_lines
      end
    end
    
    # Calculate the coverage percentage for source files only
    if total_source_coverable_lines > 0
      return {
        percentage: ((total_source_covered_lines.to_f / total_source_coverable_lines) * 100).round(2),
        covered_lines: total_source_covered_lines,
        coverable_lines: total_source_coverable_lines
      }
    end
  rescue => e
    puts "Error extracting coverage: #{e.message}"
  end
  
  return nil
end

# Custom method to print coverage badge
def coverage_badge(percentage)
  if percentage.nil?
    return "⚠️ No coverage data"
  end
  
  # Determine color based on percentage
  if percentage >= 75
    return "🟢 #{percentage}%"
  elsif percentage >= 50
    return "🟡 #{percentage}%"
  else
    return "🔴 #{percentage}%"
  end
end

# Custom method to get changed files between branches
def get_changed_files(base_branch)
  # Get the list of changed files
  changed_files = sh("git diff --name-only origin/#{base_branch}", log: false).split("\n")
  # Filter only files in Packages directory
  changed_files.select { |file| file.start_with?("Packages/") }
end

# Custom method to get package name from file path
def get_package_from_path(file_path)
  # Extract the package path after 'Packages/'
  match = file_path.match(/^Packages\/(.+?)\/(?:Sources|Tests)\//) 
  return nil unless match
  match[1] # This will return the full package path (e.g. 'CoreLayer/APIModels')
end

# Custom method to check if a package has tests
def has_tests?(package_name)
  # Make path relative to the root directory, not the fastlane directory
  root_dir = File.expand_path('..', Dir.pwd)
  base_path = File.join(root_dir, "Packages/#{package_name}")
  minimal_output("Checking for tests in: #{base_path}")
  minimal_output("Tests directory exists? #{Dir.exist?("#{base_path}/Tests")}")
  minimal_output("Current directory: #{Dir.pwd}")
  
  return true if Dir.exist?("#{base_path}/Tests")
  
  # Check for tests in subpackages
  subpackage_tests = Dir.glob("#{base_path}/*/Tests")
  minimal_output("Subpackage tests found: #{subpackage_tests}")
  subpackage_tests.any?
end

platform :ios do
  desc "Run tests only for changed packages"
  lane :test_changed_packages do |options|
    # Get base branch (default to main)
    base_branch = options[:base_branch] || "main"
    
    # Get verbosity level (default: minimal)
    verbose = options[:verbose] || "minimal"
    
    # Configure output based on verbosity
    if verbose == "minimal"
      FastlaneCore::Globals.verbose = false
      ENV["FASTLANE_SKIP_ACTION_SUMMARY"] = "true"
      FastlaneCore::UI.disable_colors = true if FastlaneCore::Helper.ci?
      ENV["FASTLANE_HIDE_TIMESTAMP"] = "true"
      ENV["FASTLANE_HIDE_DEVICE_TIMESTAMP"] = "true"
    end
    
    # Get changed files
    changed_files = get_changed_files(base_branch)
    
    # Extract unique package names
    changed_packages = changed_files.map { |file| get_package_from_path(file) }.compact.uniq
    
    minimal_output("🔍 Found #{changed_packages.length} changed packages: #{changed_packages.join(", ")}")
    
    if !changed_packages.empty?
      # Run tests for changed packages
      test_all_packages(
        packages: changed_packages,
        verbose: verbose
      )
    end
  end

  desc "Run tests for all packages or specific packages"
  lane :test_all_packages do |options|
    # Get the packages to test (if specified)
    packages_to_test = options[:packages]
    
    # Get verbosity level (default: minimal)
    verbose = options[:verbose] || "minimal"
    
    # Configure output based on verbosity
    if verbose == "minimal"
      # Disable fastlane verbosity
      FastlaneCore::Globals.verbose = false
      
      # Disable summary
      ENV["FASTLANE_SKIP_ACTION_SUMMARY"] = "true"
      
      # Disable other fastlane output
      FastlaneCore::UI.disable_colors = true if FastlaneCore::Helper.ci?
      
      # Disable step output
      ENV["FASTLANE_HIDE_TIMESTAMP"] = "true"
      ENV["FASTLANE_HIDE_DEVICE_TIMESTAMP"] = "true"
      
      # Suppress initial fastlane output
      suppress_output do
        UI.message("Starting test process...")
      end
      
      if packages_to_test && !packages_to_test.empty?
        minimal_output("🚀 Running tests for specified packages: #{packages_to_test.join(", ")}")
      else
        minimal_output("🚀 Running tests for all packages...")
      end
    else
      FastlaneCore::Globals.verbose = true
      ENV["FASTLANE_SKIP_ACTION_SUMMARY"] = "false"
    end
    
    # Find all package directories
    core_packages = Dir.glob("../Packages/CoreLayer/*").select { |f| File.directory?(f) }
    domain_packages = Dir.glob("../Packages/DomainLayer/*").select { |f| File.directory?(f) }
    presentation_packages = Dir.glob("../Packages/PresentationLayer/*").select { |f| File.directory?(f) }
    
    all_packages = core_packages + domain_packages + presentation_packages
    
    # Filter packages if a specific list was provided
    if packages_to_test && !packages_to_test.empty?
      # Convert package paths to match format in packages_to_test (e.g. 'CoreLayer/APIModels')
      filtered_packages = all_packages.select do |package_path|
        # Extract the package hierarchy (e.g. from '/path/to/Packages/CoreLayer/APIModels' to 'CoreLayer/APIModels')
        parts = package_path.split('Packages/').last.split('/')
        if parts.length >= 2
          # For nested packages like CoreLayer/APIModels
          package_id = [parts[0], parts[1]].join('/')
          packages_to_test.include?(package_id)
        else
          # For top-level packages
          packages_to_test.include?(parts[0])
        end
      end
      all_packages = filtered_packages
    end
    
    # Track test results
    results = {
      passed: [],
      failed: [],
      skipped: []
    }
    
    # Create the output directory at the project root level
    project_root = File.expand_path("../..", __FILE__)
    FileUtils.mkdir_p(File.join(project_root, "test_output"))
    
    # Track total test counts
    total_tests_run = 0
    total_tests_passed = 0
    total_tests_failed = 0
    
    # Run tests for each package
    all_packages.each do |package_dir|
      package_name = File.basename(package_dir)
      
      begin
        # Test the package directly with xcodebuild
        Dir.chdir(package_dir) do
          # Check if Package.swift exists
          unless File.exist?("Package.swift")
            UI.message("Skipping #{package_name} - no Package.swift found") if verbose != "minimal"
            results[:skipped] << { name: package_name, reason: "No Package.swift found" }
            next
          end
          
          # Check if the package has tests
          has_tests = Dir.exist?("Tests") || Dir.glob("Sources/*/Tests").any? || Dir.glob("*/Tests").any?
          unless has_tests
            UI.message("Skipping #{package_name} - no tests found") if verbose != "minimal"
            results[:skipped] << { name: package_name, reason: "No tests found" }
            next
          end
          
          # Announce test start
          if verbose == "minimal"
            minimal_output("▶️ Testing #{package_name}...")
          else
            UI.message("Running tests for package: #{package_name}")
          end
          
          # Define result bundle path at the project root level
          result_bundle_path = File.join(project_root, "test_output", "#{package_name}.xcresult")
          
          # Remove any existing result bundle
          FileUtils.rm_rf(result_bundle_path) if File.exist?(result_bundle_path)
          
          # Create a temporary file to capture the output
          output_file = Tempfile.new(["#{package_name}-test", ".log"])
          
          # Run tests using xcodebuild with SPM integration and pipe through xcpretty
          destination = "platform=iOS Simulator,name=iPhone 16 Pro,OS=latest"
          
          # Adjust xcpretty output based on verbosity
          xcpretty_format = verbose == "full" ? "" : "--simple"
          
          # Add code coverage option
          test_command = "set -o pipefail && xcodebuild test -scheme #{package_name} -destination '#{destination}' -resultBundlePath '#{result_bundle_path}' -enableCodeCoverage YES"
          
          # Add output redirection based on verbosity
          if verbose == "minimal"
            test_command += " > #{output_file.path} 2>&1"
            
            # Execute command with suppressed output
            suppress_output do
              begin
                sh(test_command)
                test_success = true
              rescue => e
                test_success = false
              end
            end
          else
            test_command += " | tee #{output_file.path} | xcpretty --color #{xcpretty_format} --report junit"
            begin
              sh(test_command)
              test_success = true
            rescue => e
              test_success = false
            end
          end
          
          # Read the output file to estimate test counts
          output_content = File.read(output_file.path)
          
          # Parse the output to get test counts
          # Look for patterns like "Executed 5 tests, with 0 failures"
          test_count_match = output_content.match(/Executed (\d+) tests?, with (\d+) failures/)
          
          tests_count = 0
          tests_failed = 0
          
          if test_count_match
            tests_count = test_count_match[1].to_i
            tests_failed = test_count_match[2].to_i
          else
            # If we can't find the pattern, check if test failed
            if !test_success || output_content.include?("** TEST FAILED **")
              tests_count = 1
              tests_failed = 1
            else
              # If we can't find the pattern, assume at least 1 test passed
              tests_count = 1
              tests_failed = 0
            end
          end
          
          tests_passed = tests_count - tests_failed
          
          # Update total counts
          total_tests_run += tests_count
          total_tests_passed += tests_passed
          total_tests_failed += tests_failed
          
          # Clean up the temporary file
          output_file.close
          output_file.unlink
          
          # Clean up build folder
          FileUtils.rm_rf("build") if Dir.exist?("build")
          
          # Extract coverage data
          coverage_data = extract_coverage_from_xcresult(result_bundle_path)
          
          # Add coverage data to results
          if tests_failed > 0
            results[:failed] << { 
              name: package_name, 
              tests_count: tests_count,
              tests_failed: tests_failed,
              tests_passed: tests_passed,
              coverage: coverage_data
            }
          else
            results[:passed] << { 
              name: package_name, 
              tests_count: tests_count,
              tests_failed: tests_failed,
              tests_passed: tests_passed,
              coverage: coverage_data
            }
          end
          
          if tests_failed > 0
            if verbose == "minimal"
              # Check for test failure
              if output_content.include?("** TEST FAILED **")
                minimal_output("❌ Test Failed")
              end
              minimal_output("❌ #{package_name}: #{tests_passed}/#{tests_count} tests passed (#{tests_failed} failed)")
              minimal_output("   Coverage: #{coverage_badge(coverage_data ? coverage_data[:percentage] : nil)}")
              # Return non-zero exit code for CI systems
              UI.user_error!("Tests failed for #{package_name}")
            else
              UI.error("❌ Tests for #{package_name} failed!")
              UI.error("   ❌ #{tests_passed}/#{tests_count} tests passed (#{tests_failed} failed)")
              UI.error("   Coverage: #{coverage_badge(coverage_data ? coverage_data[:percentage] : nil)}")
              # Return non-zero exit code for CI systems
              UI.user_error!("Tests failed for #{package_name}")
            end
          else
            if verbose == "minimal"
              # Check if test succeeded
              if output_content.include?("TEST SUCCEEDED")
                minimal_output("▸ Test Succeeded")
              end
              minimal_output("✅ #{package_name}: #{tests_passed}/#{tests_count} tests passed")
              minimal_output("   Coverage: #{coverage_badge(coverage_data ? coverage_data[:percentage] : nil)}")
            else
              UI.success("🎉 Tests for #{package_name} completed successfully!")
              UI.success("   ✅ #{tests_passed}/#{tests_count} tests passed")
              UI.success("   Coverage: #{coverage_badge(coverage_data ? coverage_data[:percentage] : nil)}")
            end
          end
        end
      rescue => e
        UI.error("Error testing package #{package_name}: #{e.message}")
        results[:failed] << { 
          name: package_name, 
          tests_count: 0,
          tests_failed: 0,
          tests_passed: 0,
          error: e.message
        }
        total_tests_failed += 1
      end
    end
    
    # Print a pretty summary
    if verbose == "minimal"
      # Display a simplified summary for minimal verbosity
      minimal_output("\n📊 Test Results Summary")
      
      if !results[:passed].empty?
        minimal_output("✅ Passed: #{results[:passed].count} packages, #{total_tests_passed} tests")
        results[:passed].each do |package|
          minimal_output("  • #{package[:name]} - #{package[:tests_passed]}/#{package[:tests_count]} tests")
          minimal_output("    Coverage: #{coverage_badge(package[:coverage] ? package[:coverage][:percentage] : nil)}")
        end
      end
      
      if !results[:failed].empty?
        minimal_output("❌ Failed: #{results[:failed].count} packages, #{total_tests_failed} tests")
        results[:failed].each do |package|
          minimal_output("  • #{package[:name]} - #{package[:tests_passed]}/#{package[:tests_count]} tests passed (#{package[:tests_failed]} failed)")
          minimal_output("    Coverage: #{coverage_badge(package[:coverage] ? package[:coverage][:percentage] : nil)}")
        end
      end
      
      if !results[:skipped].empty?
        minimal_output("⏭️ Skipped: #{results[:skipped].count} packages")
        results[:skipped].each do |package|
          minimal_output("  • #{package[:name]} - #{package[:reason]}")
        end
      end
      
      minimal_output("\n📈 Overall Statistics")
      minimal_output("Total tests: #{total_tests_run}")
      minimal_output("Passed: #{total_tests_passed}")
      minimal_output("Failed: #{total_tests_failed}")
      
      if !results[:failed].empty?
        minimal_output("❌ Some tests failed. Please check the logs for details.")
        UI.user_error!("Tests failed for #{results[:failed].map { |p| p[:name] }.join(', ')}")
      else
        minimal_output("🎉 All tests passed successfully!")
      end
    else
      UI.header("📊 Test Results Summary")
      
      # Only show passed section if there are passed tests
      if !results[:passed].empty?
        UI.success("✅ Passed (#{results[:passed].count} packages, #{total_tests_passed} tests):")
        results[:passed].each do |package|
          tests_info = "#{package[:tests_passed]}/#{package[:tests_count]} tests"
          coverage_info = package[:coverage] ? "#{coverage_badge(package[:coverage][:percentage])}" : "⚠️ No coverage data"
          UI.success("  • #{package[:name]} - #{tests_info}")
          UI.success("    Coverage: #{coverage_info}")
        end
      end
      
      # Only show failed section if there are failed tests
      if !results[:failed].empty?
        UI.error("❌ Failed (#{results[:failed].count} packages, #{total_tests_failed} tests):")
        results[:failed].each do |package|
          tests_info = "#{package[:tests_passed]}/#{package[:tests_count]} tests passed (#{package[:tests_failed]} failed)"
          coverage_info = package[:coverage] ? "#{coverage_badge(package[:coverage][:percentage])}" : "⚠️ No coverage data"
          UI.error("  • #{package[:name]} - #{tests_info}")
          UI.error("    Coverage: #{coverage_info}")
        end
      end
      
      # Only show skipped section if there are skipped packages
      if !results[:skipped].empty?
        UI.important("⏭️ Skipped (#{results[:skipped].count}):")
        results[:skipped].each do |package|
          UI.important("  • #{package[:name]} - #{package[:reason]}")
        end
      end
      
      # Final summary
      UI.header("📈 Overall Statistics")
      UI.message("Total tests: #{total_tests_run}")
      UI.message("Passed: #{total_tests_passed}")
      UI.message("Failed: #{total_tests_failed}")
      
      if results[:failed].empty?
        UI.success("🎉 All tests passed successfully!")
      else
        UI.error("❌ Some tests failed. Please check the logs for details.")
        UI.user_error!("Tests failed for #{results[:failed].map { |p| p[:name] }.join(', ')}")
      end
    end
  end
  
  desc "Run tests for a specific scheme"
  lane :test_scheme do |options|
    scheme_name = options[:scheme]
    
    unless scheme_name
      UI.user_error!("Please provide a scheme name using the 'scheme' parameter")
    end
    
    # Get verbosity level (default: minimal)
    verbose = options[:verbose] || "minimal"
    
    # Configure output based on verbosity
    if verbose == "minimal"
      # Disable fastlane verbosity
      FastlaneCore::Globals.verbose = false
      
      # Disable summary
      ENV["FASTLANE_SKIP_ACTION_SUMMARY"] = "true"
      
      # Disable other fastlane output
      FastlaneCore::UI.disable_colors = true if FastlaneCore::Helper.ci?
      
      # Disable step output
      ENV["FASTLANE_HIDE_TIMESTAMP"] = "true"
      ENV["FASTLANE_HIDE_DEVICE_TIMESTAMP"] = "true"
      
      # Suppress initial fastlane output
      suppress_output do
        UI.message("Starting test process...")
      end
      
      minimal_output("🚀 Testing #{scheme_name}...")
    else
      FastlaneCore::Globals.verbose = true
      ENV["FASTLANE_SKIP_ACTION_SUMMARY"] = "false"
    end
    
    # Find the package directory
    package_dir = nil
    
    # Search in all layer directories
    ["CoreLayer", "DomainLayer", "PresentationLayer"].each do |layer|
      potential_dir = "../Packages/#{layer}/#{scheme_name}"
      if Dir.exist?(potential_dir)
        package_dir = potential_dir
        break
      end
    end
    
    unless package_dir
      UI.user_error!("Package '#{scheme_name}' not found in any layer")
    end
    
    # Create the output directory at the project root level
    project_root = File.expand_path("../..", __FILE__)
    FileUtils.mkdir_p(File.join(project_root, "test_output"))
    
    # Test the package directly with xcodebuild
    Dir.chdir(package_dir) do
      # Check if Package.swift exists
      unless File.exist?("Package.swift")
        UI.user_error!("No Package.swift found in #{scheme_name}")
      end
      
      # Check if the package has tests
      has_tests = Dir.exist?("Tests") || Dir.glob("Sources/*/Tests").any? || Dir.glob("*/Tests").any?
      unless has_tests
        UI.user_error!("No tests found for package #{scheme_name}")
      end
      
      # Define result bundle path at the project root level
      result_bundle_path = File.join(project_root, "test_output", "#{scheme_name}.xcresult")
      
      # Remove any existing result bundle
      FileUtils.rm_rf(result_bundle_path) if File.exist?(result_bundle_path)
      
      # Create a temporary file to capture the output
      output_file = Tempfile.new(["#{scheme_name}-test", ".log"])
      
      # Run tests using xcodebuild with SPM integration and pipe through xcpretty
      destination = "platform=iOS Simulator,name=iPhone 16 Pro,OS=latest"
      
      # Adjust xcpretty output based on verbosity
      xcpretty_format = verbose == "full" ? "" : "--simple"
      
      # Add code coverage option
      test_command = "set -o pipefail && xcodebuild test -scheme #{scheme_name} -destination '#{destination}' -resultBundlePath '#{result_bundle_path}' -enableCodeCoverage YES"
      
      # Add output redirection based on verbosity
      if verbose == "minimal"
        test_command += " > #{output_file.path} 2>&1"
        
        # Execute command with suppressed output
        suppress_output do
          begin
            sh(test_command)
            test_success = true
          rescue => e
            test_success = false
          end
        end
      else
        test_command += " | tee #{output_file.path} | xcpretty --color #{xcpretty_format} --report junit"
        begin
          sh(test_command)
          test_success = true
        rescue => e
          test_success = false
        end
      end
      
      # Read the output file to estimate test counts
      output_content = File.read(output_file.path)
      
      # Parse the output to get test counts
      # Look for patterns like "Executed 5 tests, with 0 failures"
      test_count_match = output_content.match(/Executed (\d+) tests?, with (\d+) failures/)
      
      tests_count = 0
      tests_failed = 0
      
      if test_count_match
        tests_count = test_count_match[1].to_i
        tests_failed = test_count_match[2].to_i
      else
        # If we can't find the pattern, check if test failed
        if !test_success || output_content.include?("** TEST FAILED **")
          tests_count = 1
          tests_failed = 1
        else
          # If we can't find the pattern, assume at least 1 test passed
          tests_count = 1
          tests_failed = 0
        end
      end
      
      tests_passed = tests_count - tests_failed
      
      # Clean up the temporary file
      output_file.close
      output_file.unlink
      
      # Clean up build folder
      FileUtils.rm_rf("build") if Dir.exist?("build")
      
      # Extract coverage data
      coverage_data = extract_coverage_from_xcresult(result_bundle_path)
      
      if tests_failed > 0
        if verbose == "minimal"
          # Check for test failure
          if output_content.include?("** TEST FAILED **")
            minimal_output("❌ Test Failed")
          end
          minimal_output("❌ #{scheme_name}: #{tests_passed}/#{tests_count} tests passed (#{tests_failed} failed)")
          minimal_output("   Coverage: #{coverage_badge(coverage_data ? coverage_data[:percentage] : nil)}")
          # Return non-zero exit code for CI systems
          UI.user_error!("Tests failed for #{scheme_name}")
        else
          UI.error("❌ Tests for #{scheme_name} failed!")
          UI.error("   ❌ #{tests_passed}/#{tests_count} tests passed (#{tests_failed} failed)")
          UI.error("   Coverage: #{coverage_badge(coverage_data ? coverage_data[:percentage] : nil)}")
          # Return non-zero exit code for CI systems
          UI.user_error!("Tests failed for #{scheme_name}")
        end
      else
        if verbose == "minimal"
          # Check if test succeeded
          if output_content.include?("TEST SUCCEEDED")
            minimal_output("▸ Test Succeeded")
          end
          minimal_output("✅ #{scheme_name}: #{tests_passed}/#{tests_count} tests passed")
          minimal_output("   Coverage: #{coverage_badge(coverage_data ? coverage_data[:percentage] : nil)}")
        else
          UI.success("🎉 Tests for #{scheme_name} completed successfully!")
          UI.success("   ✅ #{tests_passed}/#{tests_count} tests passed")
          UI.success("   Coverage: #{coverage_badge(coverage_data ? coverage_data[:percentage] : nil)}")
        end
      end
    end
  end
end
